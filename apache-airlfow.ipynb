{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install apache-airflow","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from airflow import DAG\nfrom airflow.operators.python import PythonOperator\nfrom airflow.utils.dates import days_ago\nimport os\nimport time\nfrom glob import glob\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, Conv2DTranspose, Concatenate, Input\nfrom tensorflow.keras.models import Model\n\nfrom tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, ReduceLROnPlateau, EarlyStopping, TensorBoard\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.metrics import Recall, Precision,Accuracy\n\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\nimport numpy as np\nimport cv2\nfrom glob import glob\nfrom sklearn.utils import shuffle\nimport tensorflow as tf\nH=256\nW=256\ndim=(H,W)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-14T15:25:14.047096Z","iopub.execute_input":"2023-06-14T15:25:14.047494Z","iopub.status.idle":"2023-06-14T15:25:14.056503Z","shell.execute_reply.started":"2023-06-14T15:25:14.047463Z","shell.execute_reply":"2023-06-14T15:25:14.055332Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"dag = DAG(\n    dag_id='data_loading_and_preprocessing',\n    start_date=days_ago(1),\n    schedule_interval=None\n)\n\n# Define the tasks\ndef load_data_task():\n    path = \"/kaggle/input/kidneykits19/PNG_Slices_Segmented/PNG_Slices_Segmented\"\n    images = sorted(glob(f\"{path}/*/Images/*.jpg\"))\n    segmentations = sorted(glob(f\"{path}/*/Segmentation/*.png\"))\n    print(len(images), len(segmentations))\n    \n    split = 0.3\n    split_size = int(len(images) * split)\n    train_x, valid_x = train_test_split(images, test_size=split_size, random_state=42)\n    train_y, valid_y = train_test_split(segmentations, test_size=split_size, random_state=42)\n\n    return (train_x, train_y), (valid_x, valid_y)\n\nload_data_task = PythonOperator(\n    task_id='load_data_task',\n    python_callable=load_data_task,\n    dag=dag\n)\n\ndef read_image(path):\n    path = path.decode()\n    x = cv2.imread(path, cv2.IMREAD_COLOR)\n    x = cv2.resize(x, dim)\n    x = x/255.0\n    x = x.astype(np.float32)\n    return x\n\ndef read_mask(path):\n    path = path.decode()\n    x = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n    x = cv2.resize(x, dim)\n    x = x/255.0\n    x = x > 0.5\n    x = x.astype(np.float32)\n    x = np.expand_dims(x, axis=-1)\n    return x\n\ndef shuffling(x, y):\n    x, y = shuffle(x, y, random_state=42)\n    return x, y\n\ndef tf_parse(x, y):\n    def _parse(x, y):\n        x = read_image(x)\n        y = read_mask(y)\n        return x, y\n\n    x, y = tf.numpy_function(_parse, [x, y], [tf.float32, tf.float32])\n    x.set_shape([H, W, 3])\n    y.set_shape([H, W, 1])\n    return x, y\n\ndef tf_dataset(x, y, batch=8):\n    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n    dataset = dataset.map(tf_parse)\n    dataset = dataset.batch(batch)\n    dataset = dataset.prefetch(10)\n    return dataset\n\ntrain_x = []\ntrain_y = []\nvalid_x = []\nvalid_y = []\n\ndef preprocess_data_task(**context):\n    (train_x, train_y), (valid_x, valid_y) = context['task_instance'].xcom_pull(task_ids='load_data_task')\n    train_x, train_y = shuffling(train_x, train_y)\n    valid_x, valid_y = shuffling(valid_x, valid_y)\n    train_dataset = tf_dataset(train_x, train_y)\n    valid_dataset = tf_dataset(valid_x, valid_y)\n    # Use the processed data as needed\n\npreprocess_data_task = PythonOperator(\n    task_id='preprocess_data_task',\n    python_callable=preprocess_data_task,\n    provide_context=True,\n    dag=dag)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-06-14T16:20:37.371539Z","iopub.execute_input":"2023-06-14T16:20:37.371981Z","iopub.status.idle":"2023-06-14T16:20:37.405665Z","shell.execute_reply.started":"2023-06-14T16:20:37.371950Z","shell.execute_reply":"2023-06-14T16:20:37.404375Z"},"trusted":true},"execution_count":32,"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1;33m/tmp/ipykernel_152/\u001b[0m\u001b[1;33m1220400147.py\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m3\u001b[0m\u001b[1;33m RemovedInAirflow3Warning\u001b[0m\u001b[33m: Function `days_ago` is deprecated and will be removed in Airflow \u001b[0m\u001b[1;33m3.0\u001b[0m\u001b[33m. You can achieve equivalent behavior with `\u001b[0m\u001b[1;33mpendulum.today\u001b[0m\u001b[1;33m(\u001b[0m\u001b[33m'UTC'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.add\u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mdays\u001b[0m\u001b[33m=-N, \u001b[0m\u001b[33m...\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m`\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">/tmp/ipykernel_152/1220400147.py:</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">3</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> RemovedInAirflow3Warning</span><span style=\"color: #808000; text-decoration-color: #808000\">: Function `days_ago` is deprecated and will be removed in Airflow </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">3.0</span><span style=\"color: #808000; text-decoration-color: #808000\">. You can achieve equivalent behavior with `</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">pendulum.today(</span><span style=\"color: #808000; text-decoration-color: #808000\">'UTC'</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">).add(</span><span style=\"color: #808000; text-decoration-color: #808000\">days</span><span style=\"color: #808000; text-decoration-color: #808000\">=-N, ...</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">)</span><span style=\"color: #808000; text-decoration-color: #808000\">`</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1;33m/tmp/ipykernel_152/\u001b[0m\u001b[1;33m1220400147.py\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m1\u001b[0m\u001b[1;33m RemovedInAirflow3Warning\u001b[0m\u001b[33m: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">/tmp/ipykernel_152/1220400147.py:</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">1</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> RemovedInAirflow3Warning</span><span style=\"color: #808000; text-decoration-color: #808000\">: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1;33m/tmp/ipykernel_152/\u001b[0m\u001b[1;33m1220400147.py\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m80\u001b[0m\u001b[1;33m RemovedInAirflow3Warning\u001b[0m\u001b[33m: provide_context is deprecated as of \u001b[0m\u001b[1;33m2.0\u001b[0m\u001b[33m and is no longer required\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">/tmp/ipykernel_152/1220400147.py:</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">80</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> RemovedInAirflow3Warning</span><span style=\"color: #808000; text-decoration-color: #808000\">: provide_context is deprecated as of </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">2.0</span><span style=\"color: #808000; text-decoration-color: #808000\"> and is no longer required</span>\n</pre>\n"},"metadata":{}}]},{"cell_type":"code","source":"def Conv_Block(input,num_of_filter):\n    x=Conv2D(num_of_filter,3,padding=\"same\")(input)\n    x=BatchNormalization()(x)\n    x=Activation('ReLU')(x)\n\n    x=Conv2D(num_of_filter,3,padding=\"same\")(input)\n    x=BatchNormalization()(x)\n    x=Activation('ReLU')(x)\n    return x\n\n\ndef Encoder(input,num_of_filter):\n    x = Conv_Block(input, num_of_filter) #Skip connection\n    p = MaxPool2D((2, 2))(x)  #Feature selection\n    return x, p\n\ndef Decoder(input, skip_features, num_of_filter):\n    x = Conv2DTranspose(num_of_filter, (2, 2), strides=2, padding=\"same\")(input)\n    x = Concatenate()([x, skip_features])\n    x = Conv_Block(x, num_of_filter)\n    return x\n\n\ndef build_unet_model(input_shape):\n    inputs=Input(input_shape)\n\n  #as you go down in the encoder,resolution decreases , number of filters doubles\n    s1,p1=Encoder(inputs,32)\n    s2,p2=Encoder(p1,64)\n    s3,p3=Encoder(p2,128)\n    s4,p4=Encoder(p3,256)\n\n    b1=Conv_Block(p4,512)\n\n    d1 = Decoder(b1, s4, 256)\n    d2 = Decoder(d1, s3, 128)\n    d3 = Decoder(d2, s2, 64)\n    d4 = Decoder(d3, s1, 32)\n\n    outputs = Conv2D(1, 1, padding=\"same\", activation=\"sigmoid\")(d4)  #our data is in grayscale, gives 0,1 output\n\n    model = Model(inputs, outputs, name=\"U-Net\")\n    return model \n    \n    Build_model_task = PythonOperator(\n    task_id='build_model_task',\n    python_callable=build_unet_model,\n    provide_context=True,\n    dag=dag)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-14T16:27:26.476946Z","iopub.execute_input":"2023-06-14T16:27:26.477761Z","iopub.status.idle":"2023-06-14T16:27:26.490729Z","shell.execute_reply.started":"2023-06-14T16:27:26.477706Z","shell.execute_reply":"2023-06-14T16:27:26.489810Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"preprocess_data_task.set_upstream(load_data_task)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-14T16:34:06.618306Z","iopub.execute_input":"2023-06-14T16:34:06.618729Z","iopub.status.idle":"2023-06-14T16:34:06.627542Z","shell.execute_reply.started":"2023-06-14T16:34:06.618697Z","shell.execute_reply":"2023-06-14T16:34:06.626145Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"[\u001b[34m2023-06-14T16:34:06.620+0000\u001b[0m] {\u001b[34mtaskmixin.py:\u001b[0m211} WARNING\u001b[0m - Dependency <Task(PythonOperator): load_data_task>, preprocess_data_task already registered for DAG: data_loading_and_preprocessing\u001b[0m\n[\u001b[34m2023-06-14T16:34:06.622+0000\u001b[0m] {\u001b[34mtaskmixin.py:\u001b[0m211} WARNING\u001b[0m - Dependency <Task(PythonOperator): preprocess_data_task>, load_data_task already registered for DAG: data_loading_and_preprocessing\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"#Metrices\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import backend as K\n\ndef iou(y_true, y_pred):\n    def f(y_true, y_pred):\n        intersection = (y_true * y_pred).sum()\n        union = y_true.sum() + y_pred.sum() - intersection\n        x = (intersection + 1e-15) / (union + 1e-15)\n        x = x.astype(np.float32)\n        return x\n    return tf.numpy_function(f, [y_true, y_pred], tf.float32)\n\nsmooth = 1e-15\ndef dice_coef(y_true, y_pred):\n    y_true = tf.keras.layers.Flatten()(y_true)\n    y_pred = tf.keras.layers.Flatten()(y_pred)\n    intersection = tf.reduce_sum(y_true * y_pred)\n    return (2.0 * intersection + smooth) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) + smooth)\n\ndef dice_loss(y_true, y_pred):\n    return 1.0 - dice_coef(y_true, y_pred)","metadata":{"execution":{"iopub.status.busy":"2023-06-14T16:34:32.508308Z","iopub.execute_input":"2023-06-14T16:34:32.508770Z","iopub.status.idle":"2023-06-14T16:34:32.520447Z","shell.execute_reply.started":"2023-06-14T16:34:32.508735Z","shell.execute_reply":"2023-06-14T16:34:32.519297Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"def train_model():\n    num_epochs = 15\n    model = UNET_Build((H, W, 3))\n    metrics = [dice_coef, iou, Recall(), Precision()]\n    model.compile(loss=dice_loss, optimizer=Adam(lr), metrics=metrics)\n    model_weights = \"/kaggle/working/UNET_FYP_FINAL_15.h5\"\n    csv_weights = \"/kaggle/working/UNET_FYP_Final_15.csv\"\n\n    callbacks = [\n        ModelCheckpoint(model_weights, verbose=1, save_best_only=True),\n        ReduceLROnPlateau(monitor='val_loss', factor=0.01, patience=2, min_lr=1e-7, verbose=1),\n        TensorBoard(),\n        CSVLogger(csv_weights),\n        EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=False),\n    ]\n    start = time.time()\n    model.fit(train_dataset, epochs=num_epochs, validation_data=valid_dataset, callbacks=callbacks, shuffle=False)\n    stop = time.time()\n    print(f\"Training time: {stop - start}s\")\n    \n    \n\n    train_model_task = PythonOperator(\n        task_id='train_model',\n        python_callable=train_model\n    )\n    \n#     train_model_task","metadata":{"execution":{"iopub.status.busy":"2023-06-14T16:37:00.444083Z","iopub.execute_input":"2023-06-14T16:37:00.444792Z","iopub.status.idle":"2023-06-14T16:37:00.454212Z","shell.execute_reply.started":"2023-06-14T16:37:00.444753Z","shell.execute_reply":"2023-06-14T16:37:00.452991Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"train_model_task.set_upstream(Build_model_task)","metadata":{"execution":{"iopub.status.busy":"2023-06-14T16:39:02.522494Z","iopub.execute_input":"2023-06-14T16:39:02.522947Z","iopub.status.idle":"2023-06-14T16:39:02.573046Z","shell.execute_reply.started":"2023-06-14T16:39:02.522914Z","shell.execute_reply":"2023-06-14T16:39:02.571549Z"},"trusted":true},"execution_count":46,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[46], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_model_task\u001b[49m\u001b[38;5;241m.\u001b[39mset_upstream(Build_model_task)\n","\u001b[0;31mNameError\u001b[0m: name 'train_model_task' is not defined"],"ename":"NameError","evalue":"name 'train_model_task' is not defined","output_type":"error"}]}]}